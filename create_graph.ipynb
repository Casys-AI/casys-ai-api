{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:APOC est déjà installé et fonctionnel.\n",
      "INFO:__main__:L'index vectoriel existe probablement déjà : {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.createNodeIndex`: Caused by: org.neo4j.kernel.api.exceptions.schema.EquivalentSchemaRuleAlreadyExistsException: An equivalent index already exists, 'Index( id=3, name='entity_embeddings', type='VECTOR', schema=(:Entity {embedding}), indexProvider='vector-1.0' )'.}\n",
      "INFO:__main__:Creating or updating entities and relationships in Neo4j for crushing_mill...\n",
      "INFO:__main__:Updating embeddings...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Embeddings updated successfully.\n",
      "INFO:__main__:Finding and linking similar entities across diagrams...\n",
      "Calculating similarities: 100%|██████████| 53/53 [00:00<00:00, 67.24it/s]\n",
      "INFO:__main__:Created 15 similarity relationships.\n",
      "INFO:__main__:SysML entities and relationships stored in Neo4j for project CRUSHING_MILL.\n",
      "Calculating similarities: 100%|██████████| 124/124 [00:01<00:00, 67.06it/s]\n",
      "INFO:__main__:Created 24 similarity relationships.\n",
      "INFO:__main__:Created 24 inter-project similarity relationships.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from neo4j import GraphDatabase\n",
    "from openai import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Charger la configuration à partir du fichier YAML\n",
    "with open(\"config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# Configuration\n",
    "NEO4J_URI = config[\"neo4j\"][\"uri\"]\n",
    "NEO4J_USER = config[\"neo4j\"][\"user\"]\n",
    "NEO4J_PASSWORD = config[\"neo4j\"][\"password\"]\n",
    "OPENAI_API_KEY = config[\"openai\"][\"api_key\"]\n",
    "\n",
    "# Configuration de similarité\n",
    "EMBEDDING_WEIGHT = config[\"similarity\"][\"embedding_weight\"]\n",
    "KEYWORD_WEIGHT = config[\"similarity\"][\"keyword_weight\"]\n",
    "SIMILARITY_THRESHOLD = config[\"similarity\"][\"threshold\"]\n",
    "\n",
    "# Initialisation des clients\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def run_query(self, query: str, parameters: Dict = None) -> List[Dict]:\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, parameters or {})\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    def ensure_vector_index(self):\n",
    "        try:\n",
    "            self.run_query(\"\"\"\n",
    "            CALL db.index.vector.createNodeIndex(\n",
    "              'entity_embeddings',\n",
    "              'Entity',\n",
    "              'embedding',\n",
    "              1536,\n",
    "              'cosine'\n",
    "            )\n",
    "            \"\"\")\n",
    "            logger.info(\"Index vectoriel 'entity_embeddings' créé avec succès.\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"L'index vectoriel existe probablement déjà : {e}\")\n",
    "\n",
    "\n",
    "def batch_create_or_update_entities(tx, entities: List[Dict[str, Any]], project_name: str, project_label: str):\n",
    "    for entity in entities:\n",
    "        diagram_label = sanitize_label(entity['diagramType'])\n",
    "        entity_label = get_entity_label(entity['type'])\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        MERGE (p:Project {{name: $project_name}})\n",
    "        SET p:{sanitize_label(project_label)}\n",
    "        WITH p\n",
    "        MERGE (d:Diagram {{type: $diagram_type, project: p.name}})\n",
    "        SET d:{diagram_label}\n",
    "        MERGE (p)-[:HAS_DIAGRAM]->(d)\n",
    "        MERGE (e:Entity {{id: $entity_id}})\n",
    "        SET e += $properties\n",
    "        SET e:{entity_label}\n",
    "        MERGE (d)-[:CONTAINS_ENTITY]->(e)\n",
    "        WITH e\n",
    "        UNWIND $keywords AS keyword\n",
    "        MERGE (k:Keyword {{name: keyword}})\n",
    "        MERGE (e)-[:HAS_KEYWORD]->(k)\n",
    "        \"\"\"\n",
    "        \n",
    "        tx.run(query, {\n",
    "            'project_name': project_name,\n",
    "            'diagram_type': entity['diagramType'],\n",
    "            'entity_id': f\"{entity['diagramType']}_{entity['name']}\",\n",
    "            'properties': {\n",
    "                'name': entity['name'],\n",
    "                'type': entity['type'],\n",
    "                'description': entity['description'],\n",
    "                'keywords': entity['keywords']\n",
    "            },\n",
    "            'keywords': entity['keywords']\n",
    "        })\n",
    "\n",
    "def batch_create_relationships(tx, relationships: List[Dict[str, Any]], project_name: str):\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Project {name: $project_name})\n",
    "    WITH p\n",
    "    UNWIND $relationships AS rel\n",
    "    MATCH (s:Entity {id: rel.source_id})<-[:CONTAINS_ENTITY]-(:Diagram)<-[:HAS_DIAGRAM]-(p)\n",
    "    MATCH (t:Entity {id: rel.target_id})<-[:CONTAINS_ENTITY]-(:Diagram)<-[:HAS_DIAGRAM]-(p)\n",
    "    CALL apoc.merge.relationship(s, rel.type, {}, {}, t)\n",
    "    YIELD rel AS created_rel\n",
    "    RETURN count(created_rel)\n",
    "    \"\"\"\n",
    "    tx.run(query, {\n",
    "        'project_name': project_name,\n",
    "        'relationships': [{\n",
    "            'source_id': f\"{rel['sourceDiagram']}_{rel['source']}\",\n",
    "            'target_id': f\"{rel['targetDiagram']}_{rel['target']}\",\n",
    "            'type': rel['type'].replace(' ', '_').upper()\n",
    "        } for rel in relationships]\n",
    "    })\n",
    "\n",
    "def update_embeddings(neo4j_connection: Neo4jConnection, project_label: str = None):\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Project)-[:HAS_DIAGRAM]->(:Diagram)-[:CONTAINS_ENTITY]->(e:Entity)\n",
    "    WHERE e.description IS NOT NULL AND e.embedding IS NULL\n",
    "    \"\"\"\n",
    "    if project_label:\n",
    "        query += f\" AND p:{project_label}\"\n",
    "    query += \" RETURN e.id AS id, e.description AS description\"\n",
    "    \n",
    "    results = neo4j_connection.run_query(query)\n",
    "    \n",
    "    def process_embedding(record):\n",
    "        embedding = embeddings_model.embed_query(record['description'])\n",
    "        embedding_list = [float(x) for x in embedding]\n",
    "        update_query = \"\"\"\n",
    "        MATCH (e:Entity {id: $id})\n",
    "        SET e.embedding = $embedding\n",
    "        \"\"\"\n",
    "        neo4j_connection.run_query(update_query, {'id': record['id'], 'embedding': embedding_list})\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        list(executor.map(process_embedding, results))\n",
    "    \n",
    "    logger.info(\"Embeddings updated successfully.\")\n",
    "\n",
    "def calculate_similarities(entities: List[Dict], embedding_weight: float = EMBEDDING_WEIGHT, \n",
    "                           keyword_weight: float = KEYWORD_WEIGHT, similarity_threshold: float = SIMILARITY_THRESHOLD):\n",
    "    similarities = []\n",
    "    for i, entity1 in enumerate(tqdm(entities, desc=\"Calculating similarities\")):\n",
    "        for entity2 in entities[i+1:]:\n",
    "            if entity1['diagramType'] != entity2['diagramType']:\n",
    "                embedding_similarity = cosine_similarity(entity1['embedding'], entity2['embedding'])\n",
    "                \n",
    "                if 'keywords' in entity1 and 'keywords' in entity2:\n",
    "                    keyword_similarity = keyword_similarity_jaccard(entity1['keywords'], entity2['keywords'])\n",
    "                else:\n",
    "                    keyword_similarity = 0\n",
    "                    keyword_weight = 0\n",
    "                    embedding_weight = 1\n",
    "                \n",
    "                total_weight = embedding_weight + keyword_weight\n",
    "                embedding_weight_adjusted = embedding_weight / total_weight\n",
    "                keyword_weight_adjusted = keyword_weight / total_weight\n",
    "                \n",
    "                combined_similarity = (embedding_weight_adjusted * embedding_similarity) + (keyword_weight_adjusted * keyword_similarity)\n",
    "                \n",
    "                if combined_similarity > similarity_threshold:\n",
    "                    similarities.append({\n",
    "                        'id1': entity1['id'],\n",
    "                        'id2': entity2['id'],\n",
    "                        'similarity': combined_similarity,\n",
    "                        'embedding_similarity': embedding_similarity,\n",
    "                        'keyword_similarity': keyword_similarity\n",
    "                    })\n",
    "    return similarities\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "def keyword_similarity_jaccard(keywords1, keywords2):\n",
    "    set1 = set(keywords1)\n",
    "    set2 = set(keywords2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def create_similarity_relations(neo4j_connection: Neo4jConnection, similarities: List[Dict]):\n",
    "    query = \"\"\"\n",
    "    UNWIND $similarities AS sim\n",
    "    MATCH (e1:Entity {id: sim.id1}), (e2:Entity {id: sim.id2})\n",
    "    MERGE (e1)-[r:SIMILAR_TO]->(e2)\n",
    "    SET r.score = sim.similarity,\n",
    "        r.embedding_similarity = sim.embedding_similarity,\n",
    "        r.keyword_similarity = sim.keyword_similarity\n",
    "    \"\"\"\n",
    "    neo4j_connection.run_query(query, {'similarities': similarities})\n",
    "\n",
    "def process_similarities(neo4j_connection: Neo4jConnection, project_label: str = None):\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Project)-[:HAS_DIAGRAM]->(d:Diagram)-[:CONTAINS_ENTITY]->(e:Entity)\n",
    "    WHERE e.embedding IS NOT NULL\n",
    "    \"\"\"\n",
    "    if project_label:\n",
    "        query += f\" AND p:{project_label}\"\n",
    "    query += \"\"\"\n",
    "    RETURN e.id AS id, e.embedding AS embedding, d.type AS diagramType, \n",
    "           [(e)-[:HAS_KEYWORD]->(k) | k.name] AS keywords\n",
    "    \"\"\"\n",
    "    entities = neo4j_connection.run_query(query)\n",
    "    \n",
    "    similarities = calculate_similarities(entities)\n",
    "    create_similarity_relations(neo4j_connection, similarities)\n",
    "    \n",
    "    logger.info(f\"Created {len(similarities)} similarity relationships.\")\n",
    "    return similarities\n",
    "\n",
    "def process_json_file(file_path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return {\"entities\": [], \"relationships\": []}\n",
    "\n",
    "def rollback(neo4j_connection: Neo4jConnection, project_name: str):\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Project {name: $project_name})\n",
    "    DETACH DELETE p\n",
    "    \"\"\"\n",
    "    neo4j_connection.run_query(query, {'project_name': project_name})\n",
    "    logger.info(f\"Rollback completed for project {project_name}\")\n",
    "\n",
    "def ensure_apoc(neo4j_connection: Neo4jConnection):\n",
    "    try:\n",
    "        neo4j_connection.run_query(\"CALL apoc.help('create')\")\n",
    "        logger.info(\"APOC est déjà installé et fonctionnel.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la vérification d'APOC : {e}\")\n",
    "        logger.error(\"Assurez-vous qu'APOC est installé et activé dans votre base de données Neo4j.\")\n",
    "\n",
    "def get_file_paths(config, project_type, project_name):\n",
    "    template = config['file_templates'][project_type]\n",
    "    return {\n",
    "        diagram_type: {\n",
    "            'entities': template['entities'].format(cdc_name=project_name, part_name=project_name, diagram_type=diagram_type)\n",
    "        } for diagram_type in config['diagram_types']\n",
    "    }\n",
    "\n",
    "def sanitize_label(label: str) -> str:\n",
    "    # Remplace les caractères non alphanumériques par des underscores\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9]', '_', label)\n",
    "    # Assure que le label commence par une lettre\n",
    "    if not sanitized[0].isalpha():\n",
    "        sanitized = 'L_' + sanitized\n",
    "    return sanitized\n",
    "\n",
    "def get_entity_label(entity_type: str) -> str:\n",
    "    return sanitize_label(entity_type)\n",
    "\n",
    "def process_project(config, neo4j_connection: Neo4jConnection, project_type, project):\n",
    "    PROJECT_LABEL = project['label']\n",
    "    file_paths = get_file_paths(config, project_type, project['name'])\n",
    "    \n",
    "    all_entities = []\n",
    "    all_relationships = []\n",
    "\n",
    "    for diagram_type, paths in file_paths.items():\n",
    "        data = process_json_file(paths['entities'])\n",
    "        for entity in data['entities']:\n",
    "            entity['diagramType'] = diagram_type\n",
    "        all_entities.extend(data['entities'])\n",
    "        for relationship in data['relationships']:\n",
    "            relationship['sourceDiagram'] = diagram_type\n",
    "            relationship['targetDiagram'] = diagram_type\n",
    "        all_relationships.extend(data['relationships'])\n",
    "\n",
    "    if not all_entities or not all_relationships:\n",
    "        raise ValueError(f\"No valid entities or relationships found for {project['name']}\")\n",
    "\n",
    "    logger.info(f\"Creating or updating entities and relationships in Neo4j for {project['name']}...\")\n",
    "    with neo4j_connection.driver.session() as session:\n",
    "        session.execute_write(batch_create_or_update_entities, all_entities, project['name'], PROJECT_LABEL)\n",
    "        session.execute_write(batch_create_relationships, all_relationships, project['name'])\n",
    "\n",
    "    logger.info(\"Updating embeddings...\")\n",
    "    update_embeddings(neo4j_connection, PROJECT_LABEL)\n",
    "\n",
    "    logger.info(\"Finding and linking similar entities across diagrams...\")\n",
    "    process_similarities(neo4j_connection, PROJECT_LABEL)\n",
    "\n",
    "    logger.info(f\"SysML entities and relationships stored in Neo4j for project {PROJECT_LABEL}.\")\n",
    "\n",
    "def main(config):\n",
    "    neo4j_connection = Neo4jConnection(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "    \n",
    "    try:\n",
    "        ensure_apoc(neo4j_connection)\n",
    "        neo4j_connection.ensure_vector_index()\n",
    "\n",
    "        for project_type in ['parts']:\n",
    "            for project in config['projects'][project_type]:\n",
    "                try:\n",
    "                    process_project(config, neo4j_connection, project_type[:-1], project)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"An error occurred processing {project['name']}: {e}\")\n",
    "                    logger.info(f\"Performing rollback for {project['name']}...\")\n",
    "                    rollback(neo4j_connection, project['name'])\n",
    "\n",
    "        # Calcul des similarités entre tous les projets\n",
    "        all_similarities = process_similarities(neo4j_connection)\n",
    "        logger.info(f\"Created {len(all_similarities)} inter-project similarity relationships.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        neo4j_connection.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
